---
title: "Mehrdimensional lineare Regressionsanalyse in R mit Gradientenabstieg"
author: "Bert Niehaus - knitr4education"
date: "`r Sys.Date()`"
output: html_document
---


##  Einleitung 
Diese Seite zum Thema ''Kurs:Mehrdimensionale lineare Regression/Umsetzung in R'' kann als in unterschiedlichen Formaten angegeben werden (`html`, `odt`, `pdf` oder als Präsentation). Im Dateikopf kann man das Format durch die `ouptut`-Variable festlegen.
Einzelne Abschnitte werden als Folien betrachtet und Änderungen an den Folien wirken sich sofort auf den Inhalt der Folien aus.
Dabei werden die folgenden Teilaspekte im Detail behandelt:
* (1) Laden der Daten
* (2) Iterationen zur Fehlerminimierung
* (3) Ausgabe der Matrix und des mittleren Fehlers pro Datum

##  Zielsetzung 
Diese Lernressource hat das Ziel, die ''mehrdimensionale lineare Regression'' in R umzusetzen und die wesentlichen Schritte in Analogie zur mathematischen Herleitung zu implementieren.


##  Lernvoraussetzungen 
Die Lernressource zum Thema  ''Kurs:Mehrdimensionale lineare Regression/Umsetzung in R'' hat die folgenden Lernvoraussetzungen, die zum Verständnis der nachfolgenden Ausführungen hilfreich bzw. notwendig sind.
* '''(Mathematische Grundlagen)''' Die mathematischen Grundlagen sind wesentlich, um die Einsatzmöglichkeiten der Verfahren abschätzen zu kennen.
* '''(Grundlagen in GNU R)''' Die Grundlagen der Implementierung und [[b:de:GNU R|GNU R]] sind hilfreich, um die Implementationsschritte zu verstehen und den Code auf anderen Datenquellen ggf. anpassen zu können.

##  Aufgaben Studierende 
Bitten erzeugen Sie ein KnitR-Markdown-Dokument, indem Sie die Teilschritte als Codefragement / Codechunks einbauen und testen. Gehen Sie schrittweise vor bis Sie mit den angegeben Codezeilen ein vollständiges Verfahren zur ''mehrdimensionalen lineare Regression in R'' umgesetzt haben. Demodatei in [KnitR](https://de.wikiversity.org/wiki/KnitR) inden Sie für diese Lerneinheit in [knitr4education](https://github.com/niebert/knitr4education/tree/main/de).

##  Laden der Daten 
Als einführenden Schritt eines überwachten Lernverfahrens muss man aus den gegebenen Daten festlegen, welche Datenspalten die Eingabespalten und welche Datenspalten die Ausgabespalten der Traininfsdaten darstellen.  ''Kurs:Mehrdimensionale lineare Regression/Umsetzung in R'' dient dabei
Speichen Sie zunächst die obige [Beispieldatei](https://raw.githubusercontent.com/niebert/knitr4education/main/de/data/multlinreg.csv) `multlinreg.csv` in Ihr Verzeichnis mit dem [KnitR](https://de.wikiversity.org/wiki/KnitR)-Dokument. Das [Laden von Dateien in R](https://de.wikiversity.org/wiki/Kurs:KnitR/Laden_und_Speichern) und [KnitR](https://de.wikiversity.org/wiki/KnitR) kann bzgl. der obigen [Beispieldatei](https://raw.githubusercontent.com/niebert/knitr4education/main/de/data/multlinreg1.csv) wie folgt geschehen:
```{r , echo=TRUE }
  data <- read.csv("data/multlinreg.csv", header=TRUE, stringsAsFactors=FALSE)
```

###  CSV-Datei 
```
"x1","x2","x3","y1","y2"
1,2,3,16.05071,1.92142
2,3,1,22.06779,0.08923
7,0,4,24.96846,10.98239
7,6,5,56.06086,6.02315
```

###  Tabelle für die Daten 
In der folgenden Tabelle sind <tt>x1,x2,x3</tt> die Komponenten der Eingabevektoren und <tt>y1,y2</tt> die Komponenten der Ausgabevektoren.

## Daten in Tabelle
Mit `kable()` kann man die Daten als Tabelle anzeigen.
```{r}
knitr::kable(
  data, booktabs = TRUE,
  align=c('r','r','r','r'),
  caption = 'Erzeugte Daten für data/multlinreg.csv '
)
```
###  Bemerkung - Matrixdimension 
Die gesuchte lineare Abbildung \( A \) für die mehrdimensionale lineare Regression ist also eine  \(2 \times 3\)-Matrix. 

\[
  A:=  \begin{pmatrix}
  a_{1,1} & a_{1,2} & a_{1,3} \\
  a_{2,1} & a_{2,2} & a_{2,3} \\
 \end{pmatrix}  
\]


###  Dataframes für die Ein-/Ausgabe 
Oft enthalten die Rohdaten auch weitere Spalten, die für die [[mehrdimensionale lineare Regression]] keine Rolle spielen. Daher muss man die Ein- und Ausgabevektor aus den Rohdaten auswählen und in ein Eingabe-Dataframe und ein Ausgabe-Dataframe zusammenfassen.

###  Selektieren der Daten für x- und y-Werte 
Die obige verwendete Tabelle `multlinreg.csv`<ref name="multlinreg"/>. enthält in diesem Fall nur Spalten, die für die mehrdimensionale lineare Regression notwendig sind. Die Zuordnung der relevanten Datenspalten für die x- und y-Werte der linearen Regression wird nun gezeigt.
```{r , echo=TRUE }
  data <- read.csv("data/multlinreg.csv", header=TRUE, stringsAsFactors=FALSE)
  ## Spalten extrahieren für x_D
  x1 <- data[,1]
  x2 <- data[,2]
  x3 <- data[,3]
  ## Spalten extrahieren für y_D
  y1 <- data[,4]
  y2 <- data[,5]
  ## Dataframes für die Fehlerfunktion
  x_D <- data.frame(x1,x2,x3)
  y_D <- data.frame(y1,y2)
```

###  Bemerkung - Pfad zu Daten 
In dem Verzeichnis, in dem die R-Markdown-Dateien (Endung <tt>.Rmd</tt>) liegen, ist empfehlenswert, [Unterverzeichnisse](https://de.wikiversity.org/wiki/KnitR/Pfade) zu erzeugen:
* <tt>data/</tt> - Verzeichnis in dem alle CSV-Dateien bzw. Tabellenkalkulationsdateien liegen oder in das erzeugt Tabellen abgespeichert werden.
* <tt>lib/</tt> - Verzeichnis im dem R-Skripte und R-Bibliotheken liegen, die Sie für mehr als eine R-Markdown-Datei benötigen (z.B. <tt>knitr4education.R</tt>)

###  Ladefunktion definieren 
Für die mehrdimensionale lineare Regression muss man die relevanten Eingabespalten für die Eingabevektoren `pInCols` und relevanten Ausgabespalten für die Ausgabevektoren `pInCols` benennen. Diese werden als Vektoren der Spaltenbezeichnungen definiert:
```{r , echo=TRUE }
 ## Ladefunktion für
 load_inout_csv <- function(pFilename,pInCols,pOutCols) {
    data <- read.csv(pFilename, header=TRUE, stringsAsFactors=FALSE)
    ### Spalten mit den Bezeichnung pColNames extrahieren
    data4cols <-  list(
       xin  = data[pInCols],
       yout = data[pOutCols]
    )

    ### Rueckgabe der extrahierten Spalten
    return(data4cols)
 }
```

Mit der obigen Ladefunktion kann man die relevanten Datenspalten aus der CSV-Datei auch wie folgt in die Eingabedaten `x_ID` bzw. in die Ausgabedaten `y_ID` laden.

```{r , echo=TRUE }
 ## Spaltenbezeichnungen: Eingabevektoren IR^3
 incolnames  <- c("x1","x2","x3")
 ## Spaltenbezeichnungen: Ausgabevektoren IR^2
 outcolnames <- c("y1","y2")

 ### Input- und Outputdaten aus Datei laden
 df <- load_inout_csv("data/multlinreg.csv",incolnames,outcolnames)

 x_ID <- df$xin
 y_ID <- df$yout
```

##  MLR-Fehlerfunktion
Die LR-Fehlerfunktion \(E_{LR}(a,x_{_D},y)\) habe den Fehler für die Komponentenfunktionen berechnet, die nur einen eindimensionalen Werterbereich besitzen.

###  Zerlegung in Fehler der Komponentenfunktionen 
Die MLR-Fehlerfunktion \(E_{MLR}(A,x_{_D},y_{_D})\) zerlegt die Matrix \(A\) in Zeilenvektoren \(a_i\) und ruft die Fehlerfunktion \(E_{LR}(a_i,x_{_D},y^{(i)}) \) für die \(i\)-te Spalte \( y^{(i)}\) des Ausgabevektors in \(y_{\mathbb{D}}\) auf.

###  Aggregation der Fehler aus Komponentenfunktionen 
MLR-Fehlerfunktion \(E_{MLR}(A,x_{_D},y_{_D})\) aggregiert dann die Fehler der Komponentenfunktionen (siehe [[Mehrdimensionale lineare Regression/Zerlegung|Zerlegung in Komponentenfunktionen]] für die [[Mehrdimensionale_lineare_Regression/Gesamtfehler_aller_Fehlerfunktionen#Aggregierter_Gesamtfehler_der_Komponentenfunktionen|Berechnung des Gesamtfehlers]]).


###  LR-Fehlerfunktion für Komponentenfunktionen 
Die Bezeichnung LR-Fehlerfunktion wird verwendet, wenn man den Fehler für eine [Komponentenfunktion](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Komponentenfunktionen) berechnet, dessen Wertebereich \(\mathbb{R}\).
```{r , echo=TRUE }
E_LR <- function (pa,px_D,py1_D) {
    ## px_D : Dataframe - Liste von x-Vektoren
    ## py_D : Dataframe - Spaltenvektor von y-Werten
    ## pa : darstellender Vektor von f_a
    
    ## Fehler pro Datenpunkt
    datenanzahl <- nrow(px_D)
    e_D <- rep(0,datenanzahl)
    ## Fehler für alle Datenpunkte berechnen
    for (i in 1:datenanzahl) {
      ## quadratische Einfehler mit Funktion e
      e_D[i] <- (sum(pa*px_D[i, ]) - py1_D[i,1])^2
    }
    ## Rückgabewert als aufsummierte Einzelfehler setzen
    fehler <-  sum(e_D) ## datenanzahl
    ## Rückgabewert: return  Gesamtfehler quadratisch
    return(fehler)
  }

```

###  Gradient der LR-Fehlerfunktion für Komponenten 
Von der obigen Fehlerfunktion für einzelne [Komponentenfunktionen](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Komponentenfunktionen) muss man nun noch den Gradienten implementieren, um die [zu minimieren](Fehler)(https://de.wikiversity.org/wiki/Gradientenabstiegsverfahren).
```{r , echo=TRUE }
  GradE_LR <- function (pa,px_D,py1_D) {
    ## px_D : Dataframe - Liste von x-Vektoren
    ## py_D : Dataframe - Liste von y-Werten
    ## pa : darstellender Vektor von f_a
    
    ## Anzahl der Datenvektoren
    datenanzahl <- nrow(px_D)
    ### Gradient als Nullvektor initialisieren
    grad <- rep(0,length(pa))
    ## Fehler für alle Datenpunkte berechnen
    for (i in 1:datenanzahl) {
      ## Gradient der Summanden der Fehlerfunktion E_LR addieren
      grad <- grad + (sum(pa*px_D[i, ]) - py1_D[i, ]) * px_D[i, ]
    }
    ## Rückgabewert: ret Gradient des quadratischen Gesamtfehler as numerischen Vektor
    return(as.numeric(grad))
  }
```

###  MLR-Gesamtfehlerfunktion 
Die Gesamtfehlerfunktion \(E_{MLR}\) aggregiert alle \(m\) Fehler \(E_{LR}\) für die [Komponentenfunktionen](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Komponentenfunktionen) mit eindimensionalem Wertebereich \(\mathbb{R}\). Der Fehler wird in \(E_{MLR}\) für einen Matrix \(A \in Mat(m\times n,\mathbb{R}) \) berechnet, während \(E_{LR}\) den Fehler jeweils für einen Zeilenvektor aus der Matrix \(A\) berechnet.
```{r , echo=TRUE }
E_MLR <- function (pA,px_D,py_D) {
    ## pA : Matrix A, für die der Fehler berechnet wurde
    ## px_D : Dataframe - Liste von x-Vektoren
    ## py_D : Dataframe - Liste von y-Werten
    ## Fehler pro Datenpunkt
    cols4y <- ncol(py_D) ## Anzahl der Spalten von y_D
    fehler <- 0
    ## Fehler für alle y-Spalten
    for (i in 1:cols4y) {
      ## Fehler für Komponentenfunktion f_a berechnen
      a <- pA[i, ]
      y1_D <- py_D[i]
      fehler <- fehler + E_LR(a,px_D,y1_D)
     }
    ## Rückgabewert: fehler  quadratisch
    return(fehler)
 }
```

### Nullmatrix erzeugen
Die folgende Fu(nktion erzeugt eine Nullmatrix mit `prows` Zeilen und `pcols` Spalten
```{r}
gen0Matrix <- function (prows,pcols) {
  ### "generiere 0-Matrix"
  ### Nullvektor der Laenge prows*pcols erzeugen
  nullvec <- rep(0, prows*pcols)
  ### Nullvektor als Matrix mit prows Zeilen / pcols Spalten formatieren
  nullmat <- matrix(nullvec, nrow = prows, ncol = pcols)
  ### Rueckgabe der erzeugten Nullmatrix
  return(nullmat)
}
```


###  Gradient der MLR-Gesamtfehlerfunktion 
Auch für die Gesamtfehlerfunktion \(E_{MLR}\) benötigt man den Gradienten. Da sich der Gesamtfehler \(E_{MLR}\) aus den  [Komponentenfunktionsfehlern](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Komponentenfunktionen) \(E_{LR}\) additiv zusammensetzt, kann man mit Ableitungsregeln den Gradienten über die Gradienten von  \(E_{LR}\) berechnen.
```{r , echo=TRUE }
GradE_MLR <- function (pA,px_D,py_D) {
    ## pA : Matrix A, für die der Fehler berechnet wurde
    ## px_D : Dataframe - Liste von x-Vektoren   
    ## py_D : Dataframe - Liste von y-Werten
    
    ## Rückgabewert gradmat ist eine m x n-Matrix analog zu pA
    ## Initialisierung als Vektor der Länge 0 
    
    ## rows4grad - Anzahl der Komponentenfunktionen/Zeilen von pA 
    ##     enspricht auch der Spaltenanzahl von py_D
    rows4grad <- nrow(pA) 
    ## cols4grad - Spalten der Matrix pA 
    ##     entspricht auch der Spaltenanzahl von px_D
    cols4grad <- ncol(pA) 
    ### Gradientenmatrix gradmat als Nullmatrix initialisieren
    gradmat <- gen0Matrix(rows4grad,cols4grad)  
    
    ## Gradient für alle y-Spalten berechnen y1, y2, ...
    for (ir in 1:rows4grad) {
      ## Gradient für alle Komponentenfunktion/Matrixzeilen ermitteln
      ## a als ir-te Matrixzeile setzen (ir=Index Row)
      a <- pA[ir, ] 
      ## ir-te Vektorspalte
      y1_D <- py_D[ir]
      ### i-te Zeile der Gradientenmatrix retGrad definieren
      ### Gradientenzeile mit GradE_LR berechnen
      gradrow <- GradE_LR(a,px_D,y1_D)
      ### ir-te Zeile als numerischen Vektor in der Gradientenmatrix setzen 
      gradmat[ir, ] <- as.numeric(gradrow)
    } 
    ## Rückgabewert: grad Gradient von E_MLR als Matrix
    return(gradmat)
 }
```


###  Normierter MLR-Gradient 

Um mit der Lernrate  \(\alpha > 0 \) die Schrittweite im Parameter \(a\in \mathbb{R}^n \) kontrollieren zu können, verwendet man ein normierten Gradienten, der die Vektorlänge \(\| \mathtt{normgrad} \| = 1 \), wenn der Gradient nicht der Nullvektor ist (siehe auch [normiert LR-Gradient](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Fehlerminimierung_und_Lernrate#Normierter_LR-Gradient) ).
```{r , echo=TRUE }
normGradE_MLR <- function (pa,px_D,py_D) {
 g_LR <- GradE_MLR(pa,px_D,py_D)
 ## Länge des Gradientenvektors bestimmen als euklidische 2-Norm
 norm4g <- norm(g_LR,"2")
 normgrad <- g_LR
 if (norm4g > 0) {
    ### Normierung des Gradienten
    normgrad <- g_LR / norm4g
 } else {
   print("MLR-Gradient ist Nullvektor")
 }
 ## Rückgabewert: normierter Gradient, wenn kein Nullvektor
 ##               Sonst Nullvektor als Rückgabewert
 return(normgrad)
}
```

### Minimalen Fehler in Gradientenrichtung 
Mit der folgenden Funktion wird der minimale Fehler in Gradientenrichtung gesucht.
```{r}

find_min4error <- function (pError, pGrad, pa, px_D, py_D, alpha=1, evalcount=100) {
  ## Parameter
  ## pError: Fehlerfunktion
  ## pGrad:  Gradient der Fehlerfunktion
  ## px_D: x-Vektoren der Daten für den Definitionsbereich
  ## py_D: y-Sollwerte für die Fehlerfunktion
  ret <- rep(0,2) ### ret <- c(0,0);
  ## erste Komponente von ret ist der minimale lambda-Wert
  ## zweite Komponente von ret ist der minimale Fehler
  s4a <- (-evalcount:evalcount)/evalcount
  #s4a <- 2*runif(2*evalcount+1,-1,1)
  E4a <-  rep(0,2*evalcount+1) ## +1 wegen x4a=0
  ## smin - Skalar für Gradient für das Minimum der berechneten Fehler
  scalar4min <- 0  ### scalar4min = 0 bedeutet Gradient wird mit 0 gestreckt "man bleibt am Ort"
  ### Fehler in pa an der aktuellen Paramterposition berechnen als Startwert
  error4min <- pError( pa, px_D, py_D )
  grad4a      <- alpha * pGrad( pa, px_D, py_D )
  ## in Gradientenrichtung auswerten zwischen -1*pGrad und +1*Grad
  ## die Fehlerfunktion auswerten und über die x4a-Liste iterieren
  for (k in 1:length(s4a)) {
    ## Fehler für den um x4a[k] skalierten Gradienten pGrad
    ## und die Daten px_D, py_D berechnen
    E4a[k] <- pError(pa + s4a[k]*grad4a, px_D, py_D)
    ## Überprüfen, ob der Fehler kleiner ist als der
    ## bisher berechnete minimale Fehler errormin
    if (E4a[k] < error4min) {
      error4min  <-E4a[k]
      scalar4min <-s4a[k]
    }
  }
  # plot(s4a,E4a)
  ret <- c(scalar4min,error4min)
  ## Rückgabewert des Skalars für den Gradienten und dem minimalen Fehler
  ret
}

```

### Matrix als LaTeX
Eine Matrix in R ist eine Datenstruktur. Mathematische Ausdrücke werden sowohl im Mediawiki als auch R-Markdown in der LaTeX definiert. Die folgende Funktion erzeugt eine Matrix als Zeichenkette (String), die man in KnitR in mathematischen Formeln verwenden kann und dann die Zahlenwert der mit R berechneten Matrix enthält.
```{r}
matrix2latex <- function(pA) {
  # Überprüfe, ob pA eine Matrix ist
  if (!is.matrix(pA)) {
    stop("pA muss eine Matrix sein")
  }
  
  ### Schliesse die pmatrix-Umgebung 
  latex_string <- "\\begin{pmatrix} \n"
  #### Fuege die Zeilen der Matrix hinzu
  for (i in 1:nrow(pA)) {
    for (j in 1:ncol(pA)) {
      latex_string <- paste0(latex_string, " ", pA[i, j], " ")
      if (j < ncol(pA)) {
        latex_string <- paste0(latex_string, " & ")
      }
    }
    latex_string <- paste0(latex_string, "\\\\ \n")
  }
  
  ### Schliesse die pmatrix-Umgebung 
  latex_string <- paste0(latex_string, "\\end{pmatrix}")
  
  return(latex_string)
}
```

Nun kann man die Ausgabe der Matrix in LaTeX testen.

```{r}
# Erstelle eine Matrix
A <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2, ncol = 3)
x <- matrix(c(-1, 1, -2), nrow = 3, ncol = 1)
y <- A %*% x
# Konvertiere die Matrix A in einen LaTeX-String
latex4A <- matrix2latex(A)
latex4x <- matrix2latex(x)
latex4y <- matrix2latex(y)

str <- paste("A \\cdot x =",latex4A,"\\cdot",latex4x,"=",latex4y)
str
writeLines(str,"data/latex4matrix.tex")
```
Nun wird die Berechnung einer Matrixmultiplikation in Latex ausgegeben.
$$
  A \cdot x = `r latex4A` \cdot `r latex4x` = `r latex4y` 
$$

###  Definition der mehrdimensionalen lineare Regression 

```{r , echo=TRUE }
mdimLR <- function (pA, px_D, py_D, alpha=1, max_iteration=10, evalcount=100)  {
  ## max_iteration  maximale Anzahl der  Iterationszyklen
  ## max_iteration <- 25
  ## Lernrate alpha4it wird mit alpha initialisiert
  ## alpha4it kann sich bei der Optimierung verkleinern, 
  ## wenn sich der Fehler in einem Optimierungsschritt vergrößert.
  alterfehler <- E_MLR(pA,px_D,py_D)
  minwert <- c(0,alterfehler)
  print(paste("Error 0: ",alterfehler,sep=""))
  for (it in 1:max_iteration) {
    ### minimalen Fehler in Gradientenrichtung suchen 
    minwert <- find_min4error(E_MLR, normGradE_MLR, pA, px_D, py_D, alpha, evalcount)
    print(paste("Iteration ",it," Skalar=",minwert[1]," alpha=",alpha,sep=""))
    ### normierten Gradienten berechnen
    normgrad <- normGradE_MLR(pA, px_D, py_D)
    ### Matrix in Richtung des normierten Gradienten mit der Schrittweite minwert[1] verändern
    pA + minwert[1]* normgrad
    
    if (alterfehler > minwert[2]) {
      pA <- pA + minwert[1] * alpha * normgrad
      print(paste("Gradient",it," alpha=",alpha,sep=""))
      print(paste("Error ",it,": ",minwert[2],sep=""))
      alterfehler > minwert[2]
    } else {
      print("Fehler wird nicht kleiner - alpha halbieren")
      alpha <- alpha * 0.5
    }
  }
  ### Rückgabewert ist die Matrix mit kleinerem MLR-Fehler
  return(pA)
}
```

##  Interative Berechnung der mehrdimesionalen linearen Regression 
```{r , echo=TRUE }
## Spaltenbezeichnungen: Eingabevektoren IR^3
 incolnames  <- c("x1","x2","x3")
 ## Spaltenbezeichnungen: Ausgabevektoren IR^2
 outcolnames <- c("y1","y2")

 ## Dateiname für die Daten
 file4data <- "data/multlinreg.csv"
 
 ### Input- und Outputdaten aus Datei "data/multlinreg.csv" laden
 ### Datei befindet sich im Unterverzeichnis "data/"
 df <- load_inout_csv(file4data,incolnames,outcolnames)

 x_D <- df$xin
 y_D <- df$yout

 ## Startmatrix für die mehrdim. lin Regression setzen
 A <- matrix(c(1,2,3,4,5,6), ncol=3)
 
 ## Tests der Funktionen
 ## MLR-Gradientenmatrix
 # print("Berechnung GradE_MLR(A,x_D,y_D)")
 g0 <- GradE_MLR(A,x_D,y_D)
 ## normierte MLR-Gradientenmatrix
 g0n <- normGradE_MLR(A,x_D,y_D)
 
 e0 <- E_MLR(A,x_D,y_D)
 
 ## mehrdim lin. Regression starten
#mdimLR(A,x_D,y_D)
```

## Ausgabe der Ergebnisse vor der Optimierung 
* In der obigen Berechnung wurden die Daten aus der Datei ` `r file4data` ` geladen.
* Die Ausgangsmatrix \( A \) ist eine `r nrow(A)`x`r ncol(A)`-Matrix.

* Der Fehler \( E_{MLR} \) der Ausgangsmatrix \( A \) bzgl. der Eingabedaten \( x_{\mathbb{D}} \) und  der Ausgabedaten \( y_{\mathbb{D}} \) ist.
$$ 
  E_{MLR}(A,x_{\mathbb{D}},y_{\mathbb{D}}) = `r e0`
$$
* Nun berechnet man den Gradienten für die Ausgangsmatrix \( A \). Der Gradient  ist wie \( A \) eine `r nrow(A)`x`r ncol(A)`-Matrix.
$$
 g_{0} := Grad_A\bigg(E_{MLR}(A,x_{\mathbb{D}},y_{\mathbb{D}})\bigg) = `r matrix2latex(g0)`
$$ 
* Für die Veränderung der Parameter wird der normalisierte Gradient verwendet. Dieser wird mit der Funktion `normGrad_MLR(A,x_D,y_D)` und ist folgt definiert:
$$
  \frac{g_{0}}{\| g_{0} \|} = `r matrix2latex(g0n)``
$$
* Index 0 bedeutet, dass die Berechnungen des Fehlers vor dem ersten Optimierungsschritt erfolgt.

## Optmierungsschritt 1
* Nun berechnen man den Skalar \( \lambda_1 \in \mathbb{R} \) mit der Funktion `find_min4error()`-Funktion:
```{r}
 ## Lernrate als maximale Lernschrittweite für den normierten Gradienten
 alpha <- 10
 # print("Berechnung GradE_MLR(A,x_D,y_D)")
 g0 <- GradE_MLR(A,x_D,y_D)
 ## normierte MLR-Gradientenmatrix
 g0n <- normGradE_MLR(A,x_D,y_D)
 ### maximale Lernschrittweite
 minwert <- find_min4error(E_MLR, normGradE_MLR, A, x_D, y_D, alpha)

 A1 <- A + minwert[1] * alpha * g0n
 e1 <- minwert[2] 
```
* Mit dem Skalar \( \lambda_1 = `r minwert[1] ` \) wird der normierte Gradient \( g_{1n} \) gestreckt/gestaucht und die Ausgangsmatrix \( A \) wird wie folgt mit der Lernrate \( \alpha = `r alpha` \) verändert:
$$
  A_1 = A +(`r minwert[1] `) \cdot `r alpha ` \cdot g_{0n} 
$$
* Der neue Matrix \( A_1 \)  lautet:
$$
  A_1 =  `r matrix2latex(A1) `
$$

* Der neue Fehler \( E_{MLR}( A_1 , x_{\mathbb{D}} , y_{\mathbb{D}} ) \) Matrix lautet:
$$ 
  E_{MLR}( A_1 , x_{\mathbb{D}} , y_{\mathbb{D}} ) = `r minwert[2]`
$$
* Der Fehler wurde durch die Optimierung um `r e0 - minwert[2]` kleiner.

## Optmierungsschritt 2
* Nun berechnen man den 2. Skalar \( \lambda_2 \in \mathbb{R} \) mit der Funktion `find_min4error()`-Funktion, wober man nun als Matrix die bereits verbessert Matrix \( A_1 \) verwendet:
```{r}
minwert <- find_min4error(E_MLR, normGradE_MLR, A1, x_D, y_D, alpha=10, evalcount = 100)
 ## MLR-Gradientenmatrix  
 g1 <- GradE_MLR(A1,x_D,y_D)
 ## normierte MLR-Gradientenmatrix
 g1n <- normGradE_MLR(A1,x_D,y_D)

 A2 <- A1 + minwert[1] * alpha * g1n
 e2 <- minwert[2] 

```
* Mit dem Skalar \( \lambda = `r minwert[1] ` \) wird der normierte Gradient \( g_{2n} \) gestreckt/gestaucht, wobei nun als die bereits verbesserte Matrix \( A_1 \) gewählt. Mit dem gefundenen minimalen Wert der Fehlerfunktion wird nun die Matrix \( A_1 \) zur Matrix  \( A_2 \) verändert.
$$
  A_2 = A_1 +(`r minwert[1] `) \cdot `r alpha ` \cdot  g_{1n} 
$$
* Der neue Matrix \( A_1 \)  lautet:
$$
  A_2 =  `r matrix2latex(A2) `
$$
* Der neue Fehler \( E_{MLR}(A_2,x_{\mathbb{D}},y_{\mathbb{D}}) \) Matrix lautet:
$$ 
  E_{MLR}(A_2,x_{\mathbb{D}},y_{\mathbb{D}}) = `r minwert[2]`
$$
* Der Fehler wurde durch die Optimierung um `r e1 - minwert[2]` kleiner.

## Automatisierte Interationsschritte
```{r}
mdimLR(A, x_D, y_D, alpha=10, max_iteration=10) 

```

##  Literatur/Quellennachweise 



##  Siehe auch 
* [Gradientenabstiegsverfahren](https://de.wikiversity.org/wiki/Gradientenabstiegsverfahren)
* [Vorlagen in KnitR](https://de.wikiversity.org/wiki/KnitR/Vorlagen)
* [Zerlegung in Komponentenfunktionen](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/Zerlegung)
* [Gesamtfehler der Komponentenfunktionen](https://de.wikiversity.org/wiki/Mehrdimensionale_lineare_Regression/MLR-Gesamtfehlerfunktion)
* [Open Educational Resources](https://de.wikiversity.org/wiki/Open_Educational_Resources)

